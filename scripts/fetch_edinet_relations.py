#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
EDINETã€Œé–¢ä¿‚ä¼šç¤¾ã®çŠ¶æ³ã€å–å¾—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

æŒ‡å®šã—ãŸä¸Šå ´ä¼æ¥­ã®è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ã‚’å…ƒã«ã€EDINET APIã‹ã‚‰æœ€æ–°ã®æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸ã‚’å–å¾—ã—ã€
ã€Œé–¢ä¿‚ä¼šç¤¾ã®çŠ¶æ³ã€ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹ã€‚

å‡ºåŠ›é …ç›®: è¦ªä¼šç¤¾å, å­ä¼šç¤¾å, ä½æ‰€, è­°æ±ºæ¨©æ‰€æœ‰å‰²åˆ

ä½¿ã„æ–¹:
  python scripts/fetch_edinet_relations.py --all-listed --days 365 --use-fiscal  # æ±ºç®—æ—¥çµã‚Šè¾¼ã¿+ã‚­ãƒ£ãƒƒã‚·ãƒ¥
  python scripts/fetch_edinet_relations.py --doc-id-csv data/docid_map.csv  # DocIDå¯¾å¿œè¡¨ã§å³ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
  python scripts/fetch_edinet_relations.py 4578 2181 7270
  python scripts/fetch_edinet_relations.py --doc-id S100XXXX è¦ªä¼šç¤¾å

å¿…è¦:
  - ç’°å¢ƒå¤‰æ•° EDINET_API_KEYï¼ˆEDINET APIã‚­ãƒ¼ï¼‰
  - pip install requests pandas
"""

import argparse
import csv
import io
import json
import os
import random
import re
import shutil
import sys
import time
import zipfile
from dataclasses import dataclass
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional, Union
from xml.etree import ElementTree as ET

try:
    import requests
except ImportError:
    print("ã‚¨ãƒ©ãƒ¼: requests ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„: pip install requests")
    sys.exit(1)

# ------------------------------
# è¨­å®šï¼ˆEDINET API Version 2 ä»•æ§˜æ›¸ 2026.1ç‰ˆæº–æ‹ ï¼‰
# ------------------------------
EDINET_API_BASE = "https://disclosure.edinet-fsa.go.jp/api/v2"  # æ›¸é¡ä¸€è¦§
EDINET_DOCUMENT_BASE = "https://api.edinet-fsa.go.jp/api/v2"  # æ›¸é¡å–å¾—ï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼‰
OUTPUT_CSV = "data/edinet_relations.csv"
PROCESSED_CODES_FILE = "data/processed_codes.txt"
EDINET_CODE_LIST_PATH = "data/EdinetcodeDlInfo.csv"
EDINET_CACHE_DIR = "data/cache"  # æ›¸é¡ä¸€è¦§JSONã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆYYYY-MM-DD.jsonï¼‰
SLEEP_MIN, SLEEP_MAX = 1.0, 2.0  # æ›¸é¡ZIPå–å¾—ã”ã¨ã®å¾…æ©Ÿï¼ˆç§’ï¼‰
LIST_FETCH_SLEEP_MIN, LIST_FETCH_SLEEP_MAX = 2.0, 4.0  # æ—¥ä»˜ä¸€è¦§å–å¾—ã”ã¨ã®å¾…æ©Ÿï¼ˆç§’ï¼‰
BATCH_BREAK_COMPANIES = 100  # ã“ã®ä¼æ¥­æ•°ã”ã¨ã«é•·ã‚ã®ä¼‘æ†©
BATCH_BREAK_SEC = 60  # é•·ã‚ã®ä¼‘æ†©ï¼ˆç§’ï¼‰
REQUEST_TIMEOUT = 60  # æ›¸é¡ä¸€è¦§å–å¾—ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰
DOWNLOAD_TIMEOUT = 120  # æ›¸é¡ZIPå–å¾—ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰
DOC_TYPE_YUKASHOKEN = "120"  # æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸ï¼ˆæ–‡å­—åˆ—ã§æ¯”è¼ƒï¼‰


def is_doc_type_yukashoken(doc_type_code: Optional[Union[str, int]]) -> bool:
    """docTypeCode ãŒæœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸(120) ã‹ã€‚APIãŒæ–‡å­—åˆ—ãƒ»æ•°å€¤ã®ã©ã¡ã‚‰ã§è¿”ã—ã¦ã‚‚ä¸€è‡´ã™ã‚‹ã‚ˆã†å³å¯†ã«åˆ¤å®šã€‚"""
    if doc_type_code is None:
        return False
    return str(doc_type_code).strip() == DOC_TYPE_YUKASHOKEN


# ãƒ–ãƒ©ã‚¦ã‚¶é¢¨User-Agentï¼ˆ403å›é¿ç”¨ï¼‰
DEFAULT_USER_AGENT = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
)


@dataclass
class RelationRow:
    """é–¢ä¿‚ä¼šç¤¾1ä»¶ã®ãƒ‡ãƒ¼ã‚¿"""
    parent_name: str
    subsidiary_name: str
    address: str
    voting_rights_pct: str


def normalize_company_name(name: str) -> str:
    """ä¼æ¥­åã‚’æ­£è¦åŒ–ï¼ˆ(æ ª)â†’æ ªå¼ä¼šç¤¾ãªã©ï¼‰"""
    if not name or not isinstance(name, str):
        return ""
    return (
        name.replace("ï¼ˆæ ªï¼‰", "æ ªå¼ä¼šç¤¾")
        .replace("(æ ª)", "æ ªå¼ä¼šç¤¾")
        .replace("ãˆ±", "æ ªå¼ä¼šç¤¾")
        .replace("ï¼ˆæœ‰ï¼‰", "æœ‰é™ä¼šç¤¾")
        .replace("(æœ‰)", "æœ‰é™ä¼šç¤¾")
        .replace("ï¼ˆåˆï¼‰", "åˆè³‡ä¼šç¤¾")
        .replace("(åˆ)", "åˆè³‡ä¼šç¤¾")
        .replace("ï¼ˆåï¼‰", "åˆåä¼šç¤¾")
        .replace("(å)", "åˆåä¼šç¤¾")
        .strip()
    )


def parse_voting_rights(value: str) -> str:
    """è­°æ±ºæ¨©æ‰€æœ‰å‰²åˆã‚’æ•°å€¤æ–‡å­—åˆ—ã«æ­£è¦åŒ–"""
    if not value:
        return ""
    # ã€Œ100.00ã€ã€Œ100%ã€ã€Œ100ï¼…ã€ãªã©ã‹ã‚‰æ•°å€¤éƒ¨åˆ†ã‚’æŠ½å‡º
    m = re.search(r"(\d+(?:\.\d+)?)\s*[%ï¼…]?", str(value).strip())
    return m.group(1) if m else ""


def get_api_key() -> Optional[str]:
    return os.getenv("EDINET_API_KEY")


def load_listed_with_fiscal(path: str = EDINET_CODE_LIST_PATH) -> list[tuple[str, int]]:
    """
    EdinetcodeDlInfo.csv ã‹ã‚‰ã€Œä¸Šå ´ã€ä¼æ¥­ã® (è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰4æ¡, æ±ºç®—æœˆ) ã‚’æŠ½å‡º
    æ±ºç®—æœˆ: 1-12ï¼ˆæ±ºç®—æ—¥åˆ—ãŒã€Œ3ã€ã€Œ03ã€ã€Œ3æœˆã€ãªã©ã‹ã‚‰æŠ½å‡ºï¼‰
    """
    result: list[tuple[str, int]] = []
    p = Path(path)
    if not p.exists():
        raise FileNotFoundError(f"EDINETã‚³ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {path}")

    for enc in ("cp932", "shift_jis", "utf-8-sig", "utf-8"):
        try:
            text = p.read_text(encoding=enc)
            break
        except UnicodeDecodeError:
            continue
    else:
        raise ValueError(f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãŒåˆ¤èª­ã§ãã¾ã›ã‚“: {path}")

    lines = text.strip().splitlines()
    if len(lines) < 3:
        return []
    reader = csv.reader(io.StringIO("\n".join(lines[1:])))
    header = next(reader)
    idx_listing = next((i for i, h in enumerate(header) if "ä¸Šå ´åŒºåˆ†" in (h or "")), 2)
    idx_sec = next((i for i, h in enumerate(header) if "è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰" in (h or "")), 11)
    idx_fiscal = next((i for i, h in enumerate(header) if "æ±ºç®—" in (h or "")), 5)

    seen: set[str] = set()
    for row in reader:
        if len(row) <= max(idx_listing, idx_sec, idx_fiscal):
            continue
        listing = (row[idx_listing] or "").strip()
        sec = (row[idx_sec] or "").strip()
        fiscal_raw = (row[idx_fiscal] or "").strip()
        if listing != "ä¸Šå ´":
            continue
        if not sec or not re.match(r"^\d{4,5}$", sec):
            continue
        code = sec[:4] if len(sec) >= 4 else sec.zfill(4)
        if code in seen:
            continue
        seen.add(code)
        # æ±ºç®—æœˆã‚’æŠ½å‡ºï¼ˆã€Œ3ã€ã€Œ03ã€ã€Œ3æœˆã€ã€Œ2024-03-31ã€ãªã©ï¼‰
        fiscal_month = 3  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆ3æœˆæ±ºç®—ãŒå¤šã„ï¼‰
        if fiscal_raw:
            m = re.search(r"(\d{1,2})", fiscal_raw)
            if m:
                fm = int(m.group(1))
                if 1 <= fm <= 12:
                    fiscal_month = fm
        result.append((code, fiscal_month))
    return result

def build_dates_from_fiscal(fiscal_list: list[tuple[str, int]], months_after: int = 4) -> list[str]:
    """
    æ±ºç®—æ—¥ã‹ã‚‰ã€Œæå‡ºçª“å£ã€ã‚’ç®—å‡ºï¼ˆæ±ºç®—æœˆã®ç¿Œæœˆã€œ+months_afterãƒ¶æœˆï¼‰
    æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸ã¯æ±ºç®—å¾Œ3ã€œ4ãƒ¶æœˆä»¥å†…ã«æå‡ºã•ã‚Œã‚‹ã“ã¨ãŒå¤šã„
    """
    today = datetime.now().date()
    dates_set: set[str] = set()
    fiscal_months = {fm for _, fm in fiscal_list}
    for fm in fiscal_months:
        for year_offset in (0, 1):
            y = today.year - year_offset
            # æå‡ºçª“å£: æ±ºç®—ç¿Œæœˆ(1)ã€œæ±ºç®—+(months_after)ãƒ¶æœˆ
            for m_offset in range(1, months_after + 1):
                m = fm + m_offset
                if m > 12:
                    m -= 12
                    yy = y + 1
                else:
                    yy = y
                if yy > today.year or (yy == today.year and m > today.month):
                    continue
                last_day = 28 if m == 2 else (30 if m in (4, 6, 9, 11) else 31)
                for d in range(1, last_day + 1):
                    dt = datetime(yy, m, d).date()
                    if dt <= today:
                        dates_set.add(dt.strftime("%Y-%m-%d"))
    return sorted(dates_set, reverse=True)


def load_listed_sec_codes_from_edinet_list(path: str = EDINET_CODE_LIST_PATH) -> list[str]:
    """
    EdinetcodeDlInfo.csv ã‹ã‚‰ã€Œä¸Šå ´ã€ã‹ã¤è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ã‚ã‚Šã®4æ¡ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º
    Shift-JIS/CP932 å¯¾å¿œã€1è¡Œç›®ã¯ãƒ¡ã‚¿ã€2è¡Œç›®ã¯ãƒ˜ãƒƒãƒ€ãƒ¼
    """
    codes: list[str] = []
    p = Path(path)
    if not p.exists():
        raise FileNotFoundError(f"EDINETã‚³ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {path}")

    for enc in ("cp932", "shift_jis", "utf-8-sig", "utf-8"):
        try:
            text = p.read_text(encoding=enc)
            break
        except UnicodeDecodeError:
            continue
    else:
        raise ValueError(f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãŒåˆ¤èª­ã§ãã¾ã›ã‚“: {path}")

    lines = text.strip().splitlines()
    if len(lines) < 3:
        return []
    # 2è¡Œç›®ãŒãƒ˜ãƒƒãƒ€ãƒ¼ï¼ˆ0-indexedã§1ï¼‰ã€3è¡Œç›®ä»¥é™ãŒãƒ‡ãƒ¼ã‚¿
    reader = csv.reader(io.StringIO("\n".join(lines[1:])))
    header = next(reader)
    # åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: ä¸Šå ´åŒºåˆ†=2, è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰=11ï¼ˆä»•æ§˜ã«åŸºã¥ãï¼‰
    idx_listing = next((i for i, h in enumerate(header) if "ä¸Šå ´åŒºåˆ†" in (h or "")), 2)
    idx_sec = next((i for i, h in enumerate(header) if "è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰" in (h or "")), 11)

    for row in reader:
        if len(row) <= max(idx_listing, idx_sec):
            continue
        listing = (row[idx_listing] or "").strip()
        sec = (row[idx_sec] or "").strip()
        if listing != "ä¸Šå ´":
            continue
        if not sec or not re.match(r"^\d{4,5}$", sec):
            continue
        code = sec[:4] if len(sec) >= 4 else sec.zfill(4)
        if code not in codes:
            codes.append(code)
    return codes


def load_processed_codes(path: str = PROCESSED_CODES_FILE) -> set[str]:
    """å‡¦ç†æ¸ˆã¿è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã¿"""
    p = Path(path)
    if not p.exists():
        return set()
    return {line.strip() for line in p.read_text(encoding="utf-8").splitlines() if line.strip() and line.strip().isdigit()}


def save_processed_code(sec_code: str, path: str = PROCESSED_CODES_FILE) -> None:
    """å‡¦ç†æ¸ˆã¿è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ã‚’1ä»¶è¿½è¨˜"""
    Path(path).parent.mkdir(parents=True, exist_ok=True)
    with open(path, "a", encoding="utf-8") as f:
        f.write(sec_code + "\n")


def _default_headers() -> dict:
    """APIãƒªã‚¯ã‚¨ã‚¹ãƒˆç”¨ã®å…±é€šãƒ˜ãƒƒãƒ€ãƒ¼ï¼ˆUser-Agent ç­‰ï¼‰"""
    return {
        "User-Agent": os.getenv("EDINET_USER_AGENT", DEFAULT_USER_AGENT),
        "Accept": "application/json",
        "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
    }


def _base_params() -> dict:
    """APIã‚­ãƒ¼ã‚’ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§è¿”ã™ï¼ˆVersion 2: Subscription-Keyï¼‰ã€‚å‘¼ã³å‡ºã—å´ã§APIã‚­ãƒ¼å¿…é ˆã‚’ç¢ºèªã™ã‚‹ã“ã¨ã€‚"""
    api_key = get_api_key()
    return {"Subscription-Key": api_key} if api_key else {}


def fetch_documents_list(date_str: str) -> Optional[dict]:
    """æ›¸é¡ä¸€è¦§APIã‚’å‘¼ã³å‡ºã—ï¼ˆ/api/v2/documents.jsonï¼‰"""
    url = f"{EDINET_API_BASE}/documents.json"
    params = {"date": date_str, "type": "2", **_base_params()}
    headers = _default_headers()

    try:
        resp = requests.get(url, params=params, headers=headers, timeout=REQUEST_TIMEOUT)
        resp.raise_for_status()
        return resp.json()
    except Exception as e:
        print(f"âš ï¸ æ›¸é¡ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼ ({date_str}): {e}")
        return None


def is_valid_cache_data(data: Optional[dict]) -> bool:
    """
    æ›¸é¡ä¸€è¦§APIã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒæœ‰åŠ¹ã‹åˆ¤å®šã€‚
    results é…åˆ—ãŒå­˜åœ¨ã—ãƒªã‚¹ãƒˆã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã€‚ã‚¨ãƒ©ãƒ¼JSONã‚„ç©ºãƒ‡ãƒ¼ã‚¿ã¯ç„¡åŠ¹ã€‚
    """
    if not data or not isinstance(data, dict):
        return False
    results = data.get("results")
    return isinstance(results, list)


def ensure_dates_cached(dates: list[str], use_cache: bool = True) -> None:
    """
    STEP1&2: æŒ‡å®šæ—¥ä»˜ã®æ›¸é¡ä¸€è¦§ã‚’1æ—¥1ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§å–å¾—ã— data/cache/YYYY-MM-DD.json ã«ä¿å­˜ã€‚
    use_cache=True ã®ã¨ãæ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚¹ã‚­ãƒƒãƒ—ã€‚ç„¡åŠ¹ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯å‰Šé™¤ã—ã¦å†å–å¾—å¯¾è±¡ã«ã™ã‚‹ã€‚
    å–å¾—ã”ã¨ã« random.uniform(2.0, 4.0) ç§’å¾…æ©Ÿã€‚
    """
    cache_dir = Path(EDINET_CACHE_DIR)
    cache_dir.mkdir(parents=True, exist_ok=True)

    if use_cache:
        for date_str in dates:
            cache_file = cache_dir / f"{date_str}.json"
            if not cache_file.exists():
                continue
            try:
                data = json.loads(cache_file.read_text(encoding="utf-8"))
                if not is_valid_cache_data(data):
                    cache_file.unlink()
            except Exception:
                try:
                    cache_file.unlink()
                except Exception:
                    pass
    to_fetch = dates if not use_cache else [d for d in dates if not (cache_dir / f"{d}.json").exists()]
    if not to_fetch:
        return
    print(f"ğŸ“¡ æ›¸é¡ä¸€è¦§ã‚’å–å¾—ã—ã¾ã™ï¼ˆ{'å…¨ä»¶' if not use_cache else 'æœªã‚­ãƒ£ãƒƒã‚·ãƒ¥'} {len(to_fetch)} æ—¥ã€1æ—¥1ãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼‰...")
    for i, date_str in enumerate(to_fetch):
        data = fetch_documents_list(date_str)
        if data:
            (cache_dir / f"{date_str}.json").write_text(
                json.dumps(data, ensure_ascii=False, indent=None), encoding="utf-8"
            )
        if i < len(to_fetch) - 1:
            time.sleep(random.uniform(LIST_FETCH_SLEEP_MIN, LIST_FETCH_SLEEP_MAX))


def _normalize_sec_code(raw: Optional[Union[str, int]]) -> str:
    """
    APIã® secCodeï¼ˆ5æ¡æ•°å€¤ãƒ»æ–‡å­—åˆ—ãªã©ï¼‰ã‚’4æ¡æ–‡å­—åˆ—ã«æ­£è¦åŒ–ã€‚
    EDINETã¯ "45780" ã‚„ 45780 ã§è¿”ã™ã“ã¨ãŒã‚ã‚‹ãŸã‚ã€å…ˆé ­4æ¡ã§æ¯”è¼ƒç”¨ã«çµ±ä¸€ã€‚
    """
    s = str(raw).strip() if raw is not None else ""
    if not s:
        return ""
    # æ•°å­—ã®ã¿ã«ã—ã€å…ˆé ­4æ¡ï¼ˆä¸Šå ´éŠ˜æŸ„ã¯4æ¡ï¼‰
    digits = "".join(c for c in s if c.isdigit())
    return digits[:4].zfill(4) if digits else ""


def build_sec_to_doc_map_from_full_cache(sec_codes: set[str]) -> dict[str, dict]:
    """
    å…¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±åˆæ¤œç´¢: data/cache/*.json ã‚’ã™ã¹ã¦èª­ã¿è¾¼ã¿ã€
    è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ä¸€è‡´ã‹ã¤ docTypeCode==120ï¼ˆæœ‰å ±ï¼‰ã®æ›¸é¡ã‚’æŠ½å‡ºã€‚APIå‘¼ã³å‡ºã—ãªã—ã€‚
    5æ¡(84730)ã¨4æ¡(8473)ã®ãƒãƒƒãƒãƒ³ã‚°ã¯ _normalize_sec_code ã§å…ˆé ­4æ¡ã«çµ±ä¸€ã—ã¦æ¯”è¼ƒã€‚
    åŒä¸€è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ã«è¤‡æ•°æœ‰å ±ãŒã‚ã‚‹å ´åˆã¯ periodEnd ãŒæ–°ã—ã„ã‚‚ã®ã‚’æ¡ç”¨ã€‚
    """
    sec_set = {str(c).zfill(4) for c in sec_codes}
    result: dict[str, dict] = {}
    cache_dir = Path(EDINET_CACHE_DIR)
    if not cache_dir.exists():
        return result
    json_files = sorted(cache_dir.glob("*.json"))
    first_dumped = False
    for cache_file in json_files:
        try:
            data = json.loads(cache_file.read_text(encoding="utf-8"))
        except Exception:
            continue
        if not is_valid_cache_data(data):
            continue
        results = data.get("results", [])
        if results and not first_dumped:
            first_dumped = True
            sample = results[0]
            sd, ss = sample.get("docTypeCode"), sample.get("secCode")
            print(f"  [ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª] {cache_file.name} å…ˆé ­1ä»¶: secCode={repr(ss)} (æ­£è¦åŒ–â†’{_normalize_sec_code(ss)}), docTypeCode={repr(sd)}, is_120={is_doc_type_yukashoken(sd)}")
        for r in results:
            if not is_doc_type_yukashoken(r.get("docTypeCode")):
                continue
            sec = _normalize_sec_code(r.get("secCode"))
            if not sec or sec not in sec_set:
                continue
            # åŒä¸€è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ã§æ—¢å­˜ã‚ã‚Šï¼šperiodEnd ãŒæ–°ã—ã„æ–¹ã‚’æ¡ç”¨
            existing = result.get(sec)
            new_end = r.get("periodEnd") or ""
            if existing:
                old_end = existing.get("periodEnd") or ""
                if new_end <= old_end:
                    continue
            result[sec] = {
                "docID": r.get("docID"),
                "filerName": r.get("filerName"),
                "secCode": sec,
                "periodStart": r.get("periodStart"),
                "periodEnd": r.get("periodEnd"),
            }
    return result


def fetch_documents_list_cached(date_str: str) -> Optional[dict]:
    """
    æ›¸é¡ä¸€è¦§ã‚’å–å¾—ï¼ˆãƒ‡ã‚£ã‚¹ã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥å„ªå…ˆï¼‰ã€‚
    ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆæ™‚ã¯APIå‘¼ã³å‡ºã—ãƒ»ã‚¹ãƒªãƒ¼ãƒ—ãªã—ã€‚
    """
    cache_file = Path(EDINET_CACHE_DIR) / f"{date_str}.json"
    if cache_file.exists():
        try:
            return json.loads(cache_file.read_text(encoding="utf-8"))
        except Exception:
            pass
    _request_sleep()
    return fetch_documents_list(date_str)


def _request_sleep() -> None:
    """1ãƒªã‚¯ã‚¨ã‚¹ãƒˆã”ã¨ã®å¾…æ©Ÿï¼ˆ1ã€œ2ç§’ï¼‰"""
    time.sleep(random.uniform(SLEEP_MIN, SLEEP_MAX))


def build_sec_to_doc_map(sec_codes: set[str], dates_to_fetch: list[str], use_cache: bool = True) -> dict[str, dict]:
    """
    æŒ‡å®šæ—¥ä»˜ç¾¤ã‚’1æ—¥1ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§å–å¾—ã—ã€è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰â†’æ›¸é¡æƒ…å ±ã®ãƒãƒƒãƒ—ã‚’æ§‹ç¯‰
    ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ©ç”¨æ™‚ã¯ data/edinet_cache/YYYY-MM-DD.json ã‚’å„ªå…ˆ
    docTypeCode==120ï¼ˆæœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸ï¼‰ã®ã¿æŠ½å‡º
    """
    sec_set = {str(c).zfill(4) for c in sec_codes}
    result: dict[str, dict] = {}
    fetch_fn = fetch_documents_list_cached if use_cache else fetch_documents_list

    for i, date_str in enumerate(dates_to_fetch):
        if not use_cache:
            _request_sleep()
        data = fetch_fn(date_str)
        if not data or "results" not in data:
            continue
        for r in data.get("results", []):
            if not is_doc_type_yukashoken(r.get("docTypeCode")):
                continue
            sec = _normalize_sec_code(r.get("secCode"))
            if not sec or sec not in sec_set or sec in result:
                continue
            result[sec] = {
                "docID": r.get("docID"),
                "filerName": r.get("filerName"),
                "secCode": sec,
                "periodStart": r.get("periodStart"),
                "periodEnd": r.get("periodEnd"),
            }
    return result


def load_doc_id_csv(path: str) -> list[tuple[str, str, str]]:
    """
    DocIDå¯¾å¿œè¡¨CSVã‚’èª­ã¿è¾¼ã¿ (sec_code, doc_id, filer_name) ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
    ãƒ˜ãƒƒãƒ€ãƒ¼: sec_code,doc_id,filer_name ã¾ãŸã¯ sec_code,doc_id
    """
    p = Path(path)
    if not p.exists():
        raise FileNotFoundError(f"DocIDå¯¾å¿œè¡¨ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {path}")
    rows: list[tuple[str, str, str]] = []
    with open(p, encoding="utf-8-sig", newline="") as f:
        reader = csv.DictReader(f)
        for r in reader:
            sec = (r.get("sec_code") or r.get("è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰") or r.get("secCode") or "").strip()
            doc_id = (r.get("doc_id") or r.get("docID") or r.get("docid") or "").strip()
            filer = (r.get("filer_name") or r.get("æå‡ºè€…å") or r.get("filerName") or "").strip()
            if sec and doc_id:
                sec4 = sec[:4] if len(sec) >= 4 else sec.zfill(4)
                rows.append((sec4, doc_id, filer or f"è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰{sec4}"))
    return rows


def fetch_document_zip(doc_id: str, doc_type: str = "1") -> Optional[bytes]:
    """
    æ›¸é¡å–å¾—APIã§ZIPã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆVersion 2: api.edinet-fsa.go.jp, Subscription-Key ã‚’ã‚¯ã‚¨ãƒªã«ä»˜ä¸ï¼‰
    doc_type: 1=XBRL, 5=CSVï¼ˆä»•æ§˜æ›¸ã«åŸºã¥ãï¼‰
    """
    url = f"{EDINET_DOCUMENT_BASE}/documents/{doc_id}"
    params = {"type": doc_type, **_base_params()}
    headers = _default_headers()

    try:
        resp = requests.get(url, params=params, headers=headers, timeout=DOWNLOAD_TIMEOUT)
        resp.raise_for_status()

        content_type = (resp.headers.get("Content-Type") or "").split(";")[0].strip().lower()
        if content_type == "application/json":
            print(f"âš ï¸ æ›¸é¡å–å¾—ãŒJSONã‚’è¿”ã—ã¾ã—ãŸ (docID={doc_id}, type={doc_type})ã€‚ã‚µãƒ¼ãƒãƒ¼æ‹’å¦ã®å¯èƒ½æ€§:")
            print(resp.text[:2000] if len(resp.text) > 2000 else resp.text)
            return None

        raw = resp.content
        # ãƒã‚¤ãƒŠãƒªæ•´åˆæ€§ãƒã‚§ãƒƒã‚¯: JSONï¼ˆã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼‰ã«ãªã£ã¦ã„ãªã„ã‹å³æ ¼ã«åˆ¤å®š
        stripped = raw.strip()
        if stripped.startswith(b"{") or stripped.startswith(b"["):
            preview = raw.decode("utf-8", errors="replace")
            print(f"âš ï¸ æ›¸é¡å–å¾—ãŒJSONå½¢å¼ã‚’è¿”ã—ã¾ã—ãŸ (docID={doc_id}, type={doc_type})ã€‚ã‚¨ãƒ©ãƒ¼å¿œç­”ã®å¯èƒ½æ€§:")
            print(preview[:3000] if len(preview) > 3000 else preview)
            return None
        if not zipfile.is_zipfile(io.BytesIO(raw)):
            preview = raw.decode("utf-8", errors="replace")
            print(f"âš ï¸ ZIPå½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“ (docID={doc_id}, type={doc_type})ã€‚APIã‚¨ãƒ©ãƒ¼JSONã®å¯èƒ½æ€§:")
            print("ãƒ¬ã‚¹ãƒãƒ³ã‚¹å…ˆé ­500æ–‡å­—:", preview[:500])
            return None

        return raw
    except requests.RequestException as e:
        print(f"âš ï¸ æ›¸é¡ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼ (docID={doc_id}): {e}")
        if hasattr(e, "response") and e.response is not None and getattr(e.response, "text", None):
            print("ãƒ¬ã‚¹ãƒãƒ³ã‚¹:", e.response.text[:1500])
        return None


def fetch_document_zip_with_fallback(doc_id: str, prefer_csv: bool = True) -> tuple[Optional[bytes], str]:
    """
    type=5(CSV) ã¨ type=1(XBRL) ã‚’è©¦è¡Œã—ã¦ZIPã‚’å–å¾—ã€‚
    prefer_csv=True ã®ã¨ã CSV ã‚’å„ªå…ˆï¼ˆVersion 2æ­£å¼ã‚µãƒãƒ¼ãƒˆã€æ§‹é€ ãŒå˜ç´”ã§æŠ½å‡ºãƒŸã‚¹ãŒæ¸›ã‚‹ï¼‰ã€‚
    æˆ»ã‚Šå€¤: (zip_bytes, "csv"|"xbrl"|"") å–å¾—ã«å¤±æ•—ã—ãŸå ´åˆã¯ (None, "")
    """
    if prefer_csv:
        data = fetch_document_zip(doc_id, doc_type="5")
        if data is not None:
            return (data, "csv")
        data = fetch_document_zip(doc_id, doc_type="1")
        if data is not None:
            return (data, "xbrl")
    else:
        data = fetch_document_zip(doc_id, doc_type="1")
        if data is not None:
            return (data, "xbrl")
        data = fetch_document_zip(doc_id, doc_type="5")
        if data is not None:
            return (data, "csv")
    return (None, "")


def parse_xbrl_for_relations(zip_bytes: bytes, parent_name: str, debug: bool = False) -> list[RelationRow]:
    """
    XBRL ZIPã‹ã‚‰é–¢ä¿‚ä¼šç¤¾ã®çŠ¶æ³ã‚’æŠ½å‡º
    ã‚¿ã‚¯ã‚½ãƒãƒŸã®å­ä¼šç¤¾ãƒ»é–¢é€£ä¼šç¤¾é–¢é€£è¦ç´ ã‚’æ¢ã™
    """
    rows: list[RelationRow] = []
    namespaces = {
        "xbrli": "http://www.xbrl.org/2003/instance",
        "jpdei": "http://disclosure.edinet-fsa.go.jp/taxonomy/jpdei/2024-12-01/jpdei",
        "jpcrp": "http://disclosure.edinet-fsa.go.jp/taxonomy/jpcrp/2024-12-01/jpcrp",
        "jppfs": "http://disclosure.edinet-fsa.go.jp/taxonomy/jppfs/2024-12-01/jppfs",
        "tse": "http://www.xbrl.tdnet.info/jp/tse/tdnet/t/2024-12-01/tse-t-2024-12-01",
        "xlink": "http://www.w3.org/1999/xlink",
    }
    # é–¢ä¿‚ä¼šç¤¾ã®åç§°ãƒ»ä½æ‰€ãƒ»è­°æ±ºæ¨©ã®è¦ç´ åãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆã‚¿ã‚¯ã‚½ãƒãƒŸå¹´ç‰ˆã§å¤‰ã‚ã‚‹å¯èƒ½æ€§ã‚ã‚Šï¼‰
    name_patterns = [
        "SubsidiaryCompanyName",
        "RelatedCompanyName",
        "SubsidiaryCompanyNameOfListedCompany",
        "NameOfSubsidiaryCompany",
        "NameOfRelatedCompany",
    ]
    addr_patterns = ["SubsidiaryCompanyAddress", "RelatedCompanyAddress", "AddressOfSubsidiaryCompany"]
    ratio_patterns = [
        "VotingRightsOwnedPercent",
        "OwnershipOfVotingRights",
        "PercentageOfVotingRights",
        "EquityMethodInvestmentRatio",
    ]

    xbrl_count = 0
    try:
        with zipfile.ZipFile(io.BytesIO(zip_bytes), "r") as zf:
            for name in zf.namelist():
                if not name.lower().endswith(".xbrl") and not name.lower().endswith(".xml"):
                    continue
                xbrl_count += 1
                try:
                    content = zf.read(name)
                except Exception:
                    continue
                # åå‰ç©ºé–“ã‚’ç™»éŒ²ã—ã¦ãƒ‘ãƒ¼ã‚¹
                try:
                    root = ET.fromstring(content)
                except ET.ParseError:
                    continue

                # å…¨è¦ç´ ã‚’èµ°æŸ»ï¼ˆåå‰ç©ºé–“ä»˜ãï¼‰
                for elem in root.iter():
                    tag = elem.tag
                    if "}" in tag:
                        local = tag.split("}")[-1]
                    else:
                        local = tag

                    text = (elem.text or "").strip()
                    if not text:
                        continue

                    if any(p in local for p in name_patterns):
                        row = RelationRow(
                            parent_name=parent_name,
                            subsidiary_name=text,
                            address="",
                            voting_rights_pct="",
                        )
                        rows.append(row)
                    elif rows and any(p in local for p in addr_patterns):
                        rows[-1].address = text
                    elif rows and any(p in local for p in ratio_patterns):
                        pct = parse_voting_rights(text)
                        if pct:
                            rows[-1].voting_rights_pct = pct

    except Exception as e:
        print(f"âš ï¸ XBRLãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")

    if debug:
        print(f"    [XBRL] å‡¦ç†ã—ãŸXBRL/XMLãƒ•ã‚¡ã‚¤ãƒ«æ•°: {xbrl_count}, æŠ½å‡ºä»¶æ•°: {len(rows)}")
    return rows


def parse_csv_for_relations(zip_bytes: bytes, parent_name: str, debug: bool = False) -> list[RelationRow]:
    """
    ZIPå†…ã®XBRL_TO_CSVå½¢å¼CSVã‹ã‚‰é–¢ä¿‚ä¼šç¤¾ã‚’æŠ½å‡º
    ï¼ˆEDINETã‚µã‚¤ãƒˆã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸCSVå½¢å¼ã«å¯¾å¿œï¼‰
    """
    rows: list[RelationRow] = []
    current_subsidiary = ""
    current_address = ""
    current_ratio = ""

    csv_count = 0
    try:
        with zipfile.ZipFile(io.BytesIO(zip_bytes), "r") as zf:
            for name in zf.namelist():
                if not name.lower().endswith(".csv"):
                    continue
                csv_count += 1
                try:
                    content = zf.read(name)
                except Exception:
                    continue
                # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¤œå‡º
                for enc in ("utf-8", "utf-8-sig", "utf-16", "cp932", "shift_jis"):
                    try:
                        text = content.decode(enc)
                        break
                    except UnicodeDecodeError:
                        continue
                else:
                    continue

                # ã‚¿ãƒ–åŒºåˆ‡ã‚Šã‚’æƒ³å®š
                delim = "\t" if "\t" in text[:500] else ","
                reader = csv.DictReader(io.StringIO(text), delimiter=delim)
                if not reader.fieldnames:
                    continue

                # é …ç›®åãƒ»å€¤ã®åˆ—ã‚’ç‰¹å®š
                item_key = next((k for k in reader.fieldnames if "é …ç›®" in k or "è¦ç´ " in k), "é …ç›®å")
                value_key = next((k for k in reader.fieldnames if k in ("å€¤", "value", "Value")), "å€¤")

                for rec in reader:
                    item = (rec.get(item_key) or "").strip()
                    val = (rec.get(value_key) or "").strip()
                    if not item or not val:
                        continue

                    if "å­ä¼šç¤¾" in item or "é–¢é€£ä¼šç¤¾" in item:
                        if "åç§°" in item or "åå‰" in item or "Name" in item:
                            if current_subsidiary:
                                rows.append(
                                    RelationRow(
                                        parent_name=parent_name,
                                        subsidiary_name=normalize_company_name(current_subsidiary),
                                        address=current_address,
                                        voting_rights_pct=current_ratio,
                                    )
                                )
                            current_subsidiary = val
                            current_address = ""
                            current_ratio = ""
                        elif "æŒæ ª" in item or "è­°æ±ºæ¨©" in item or "æ¯”ç‡" in item or "EquityRatio" in item:
                            current_ratio = parse_voting_rights(val) or current_ratio
                        elif "æ‰€åœ¨åœ°" in item or "ä½æ‰€" in item or "Address" in item:
                            current_address = val

                if current_subsidiary:
                    rows.append(
                        RelationRow(
                            parent_name=parent_name,
                            subsidiary_name=normalize_company_name(current_subsidiary),
                            address=current_address,
                            voting_rights_pct=current_ratio,
                        )
                    )

    except Exception as e:
        print(f"âš ï¸ CSVãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")

    if debug:
        print(f"    [CSV] å‡¦ç†ã—ãŸCSVãƒ•ã‚¡ã‚¤ãƒ«æ•°: {csv_count}, æŠ½å‡ºä»¶æ•°: {len(rows)}")
    return rows


def extract_relations(
    zip_bytes: bytes,
    parent_name: str,
    debug: bool = False,
    fetched_format: str = "",
) -> list[RelationRow]:
    """
    ZIPã‹ã‚‰é–¢ä¿‚ä¼šç¤¾ã‚’æŠ½å‡ºã€‚
    CSVã‚’å„ªå…ˆã—ã€è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯XBRLã‚’è©¦è¡Œï¼ˆVersion 2æ­£å¼ã‚µãƒãƒ¼ãƒˆã®CSVã¯æ§‹é€ ãŒå˜ç´”ï¼‰ã€‚
    debug=True ã®ã¨ãZIPå†…ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã¨æŠ½å‡ºã‚¹ãƒ†ãƒƒãƒ—ã‚’ãƒ­ã‚°å‡ºåŠ›ã€‚
    """
    if debug:
        try:
            with zipfile.ZipFile(io.BytesIO(zip_bytes), "r") as zf:
                names = zf.namelist()
                print(f"    [å±•é–‹] ZIPå†…ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(names)}")
                for i, n in enumerate(names[:20]):
                    print(f"      - {n}")
                if len(names) > 20:
                    print(f"      ... ä»– {len(names) - 20} ä»¶")
        except Exception as e:
            print(f"    [å±•é–‹] ZIPèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

    # CSVå„ªå…ˆï¼ˆtype=5å–å¾—æ™‚ã«CSVã®ã¿å«ã¾ã‚Œã‚‹ZIPï¼‰
    rows = parse_csv_for_relations(zip_bytes, parent_name, debug=debug)
    if debug and not rows:
        print(f"    [æŠ½å‡º] CSVè§£æ: 0ä»¶ â†’ XBRLã‚’è©¦è¡Œ")
    if not rows:
        rows = parse_xbrl_for_relations(zip_bytes, parent_name, debug=debug)
    if debug:
        print(f"    [æŠ½å‡º] çµæœ: {len(rows)} ä»¶ (å–å¾—å½¢å¼: {fetched_format or 'ä¸æ˜'})")
    return rows


def append_to_csv(rows: list[RelationRow], output_file: str = OUTPUT_CSV):
    """edinet_relations.csvã«è¿½è¨˜ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼ã¯åˆå›ã®ã¿ï¼‰"""
    p = Path(output_file)
    p.parent.mkdir(parents=True, exist_ok=True)
    file_exists = p.exists()

    with open(output_file, "a", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        if not file_exists:
            w.writerow(["è¦ªä¼šç¤¾å", "å­ä¼šç¤¾å", "ä½æ‰€", "è­°æ±ºæ¨©æ‰€æœ‰å‰²åˆ"])
        for r in rows:
            w.writerow([
                normalize_company_name(r.parent_name),
                normalize_company_name(r.subsidiary_name),
                r.address,
                r.voting_rights_pct,
            ])


def main():
    parser = argparse.ArgumentParser(description="EDINETé–¢ä¿‚ä¼šç¤¾ã®çŠ¶æ³ã‚’å–å¾—")
    parser.add_argument("codes", nargs="*", help="è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ï¼ˆä¾‹: 4578 2181 7270ï¼‰")
    parser.add_argument("--codes", type=str, dest="codes_str", help="ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šè¨¼åˆ¸ã‚³ãƒ¼ãƒ‰")
    parser.add_argument("--all-listed", action="store_true", help="EdinetcodeDlInfo.csvã‹ã‚‰ä¸Šå ´ä¼æ¥­ã‚’è‡ªå‹•ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—")
    parser.add_argument("--doc-id", type=str, dest="doc_id", help="æ›¸é¡IDã‚’ç›´æ¥æŒ‡å®šï¼ˆä¸€è¦§APIã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰")
    parser.add_argument("--doc-id-csv", type=str, dest="doc_id_csv", help="è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰-DocIDå¯¾å¿œè¡¨CSVï¼ˆsec_code,doc_id,filer_nameï¼‰æ—¥ä»˜æ¤œç´¢ã‚’å®Œå…¨ã‚¹ã‚­ãƒƒãƒ—")
    parser.add_argument("--days", type=int, default=1, help="æ›¸é¡æ¤œç´¢æœŸé–“ï¼ˆéå»ä½•æ—¥åˆ†ã‚’é¡ã‚‹ã‹ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ1=ç›´è¿‘1æ—¥ã®ã¿ï¼‰")
    parser.add_argument("--use-fiscal", action="store_true", help="--all-listedæ™‚ã€æ±ºç®—æ—¥ã‹ã‚‰æå‡ºçª“å£ï¼ˆ4ãƒ¶æœˆï¼‰ã«çµã£ã¦æ¤œç´¢ï¼ˆAPIå‘¼ã³å‡ºã—å‰Šæ¸›ï¼‰")
    parser.add_argument("--no-cache", action="store_true", help="æ›¸é¡ä¸€è¦§ã®ãƒ‡ã‚£ã‚¹ã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡åŠ¹åŒ–")
    parser.add_argument("--force-refresh-cache", action="store_true", help="data/cache/ ã‚’ç„¡è¦–ã—ã€v2ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰æ›¸é¡ä¸€è¦§ã‚’å†å–å¾—ã—ã¦ä¸Šæ›¸ã")
    parser.add_argument("--clear-cache", action="store_true", help="å®Ÿè¡Œé–‹å§‹æ™‚ã« data/cache/ ã‚’å‰Šé™¤ã—ã¦ã‹ã‚‰å‡¦ç†ã‚’é–‹å§‹")
    parser.add_argument("--output", type=str, default=OUTPUT_CSV, help="å‡ºåŠ›CSVãƒ‘ã‚¹")
    parser.add_argument("--overwrite", action="store_true", help="æ—¢å­˜CSVã‚’ä¸Šæ›¸ãã—ã¦æ–°è¦ä½œæˆï¼ˆé€šå¸¸ã¯è¿½è¨˜ï¼‰")
    parser.add_argument("--no-resume", action="store_true", help="å‡¦ç†æ¸ˆã¿è¨˜éŒ²ã‚’ç„¡è¦–ã—ã¦æœ€åˆã‹ã‚‰å®Ÿè¡Œ")
    parser.add_argument("--debug", action="store_true", help="1ç¤¾ã«çµã£ã¦è©³ç´°è§£æãƒ¢ãƒ¼ãƒ‰ï¼ˆZIPå†…ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã€æŠ½å‡ºã‚¹ãƒ†ãƒƒãƒ—ã‚’ãƒ­ã‚°å‡ºåŠ›ï¼‰")
    args = parser.parse_args()

    if args.codes_str:
        codes = [c.strip() for c in args.codes_str.split(",") if c.strip()]
    elif args.all_listed:
        try:
            codes = load_listed_sec_codes_from_edinet_list()
            print(f"ğŸ“‹ EDINETã‚³ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã‹ã‚‰ä¸Šå ´ä¼æ¥­ {len(codes)} ç¤¾ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        except FileNotFoundError as e:
            print(f"âŒ {e}")
            print("   data/EdinetcodeDlInfo.csv ã‚’ EDINET ã‚µã‚¤ãƒˆã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
            sys.exit(1)
        except Exception as e:
            print(f"âŒ ã‚³ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            sys.exit(1)
    else:
        codes = [str(c).strip() for c in args.codes if c]

    # --doc-id ãƒ¢ãƒ¼ãƒ‰ / --doc-id-csv ãƒ¢ãƒ¼ãƒ‰
    doc_id_direct = (args.doc_id or "").strip()
    doc_id_csv_path = (args.doc_id_csv or "").strip()
    use_doc_id_mode = bool(doc_id_direct)
    use_doc_id_csv_mode = bool(doc_id_csv_path)

    if not use_doc_id_mode and not use_doc_id_csv_mode and not codes:
        print("è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ã€--codesã€ã¾ãŸã¯ --all-listed ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
        print("ä¾‹: python fetch_edinet_relations.py 4578 2181 7270")
        print("ä¾‹: python fetch_edinet_relations.py --all-listed --days 365")
        sys.exit(1)

    # APIã‚­ãƒ¼å¿…é ˆï¼ˆEDINET API v2: Subscription-Key ã‚’ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§é€ä¿¡ï¼‰
    if not get_api_key():
        print("âŒ ç’°å¢ƒå¤‰æ•° EDINET_API_KEY ãŒæœªè¨­å®šã§ã™ã€‚")
        print("   EDINET API ã‚­ãƒ¼å–å¾—: https://disclosure.edinet-fsa.go.jp/api/auth/index.aspx?mode=1")
        print("   è¨­å®šä¾‹: export EDINET_API_KEY=your_key")
        sys.exit(1)

    output_path = args.output

    # å‡¦ç†æ¸ˆã¿ã‚³ãƒ¼ãƒ‰ï¼ˆ--all-listed æ™‚ã®ãƒ¬ã‚¸ãƒ¥ãƒ¼ãƒ ç”¨ï¼‰
    processed = set() if args.no_resume else load_processed_codes()
    if processed and not use_doc_id_mode and not use_doc_id_csv_mode:
        codes = [c for c in codes if c not in processed]
        print(f"â­ å‡¦ç†æ¸ˆã¿ {len(processed)} ä»¶ã‚’ã‚¹ã‚­ãƒƒãƒ—ã€æ®‹ã‚Š {len(codes)} ä»¶ã‚’å‡¦ç†ã—ã¾ã™")

    # CSVã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§è¿½è¨˜ã€‚--overwrite æ™‚ã®ã¿ä¸Šæ›¸ã
    if args.overwrite and Path(output_path).exists():
        Path(output_path).unlink()
        print(f"æ—¢å­˜CSVã‚’å‰Šé™¤: {output_path}")

    total = 0
    request_count = 0

    if use_doc_id_mode:
        # æ›¸é¡IDç›´æ¥æŒ‡å®šãƒ¢ãƒ¼ãƒ‰ï¼ˆä¸€è¦§APIã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰
        parent = codes[0] if codes else "ä¸æ˜"
        doc_id = doc_id_direct
        print(f"\nğŸ“‹ æ›¸é¡IDç›´æ¥æŒ‡å®šãƒ¢ãƒ¼ãƒ‰: {doc_id} (è¦ªä¼šç¤¾å: {parent})")
        _request_sleep()
        zip_bytes, fmt = fetch_document_zip_with_fallback(doc_id)
        if zip_bytes:
            relations = extract_relations(zip_bytes, parent, debug=args.debug, fetched_format=fmt)
            seen = set()
            unique = []
            for r in relations:
                key = (r.parent_name, r.subsidiary_name, r.address, r.voting_rights_pct)
                if key not in seen and r.subsidiary_name:
                    seen.add(key)
                    unique.append(r)
            if unique:
                append_to_csv(unique, output_path)
                total = len(unique)
                print(f"  âœ… {total} ä»¶ã‚’æŠ½å‡ºãƒ»ä¿å­˜")
            else:
                print(f"  âš ï¸ é–¢ä¿‚ä¼šç¤¾ãƒ‡ãƒ¼ã‚¿ãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
    elif use_doc_id_csv_mode:
        # DocIDå¯¾å¿œè¡¨CSVãƒ¢ãƒ¼ãƒ‰ï¼ˆæ—¥ä»˜æ¤œç´¢ã‚’å®Œå…¨ã‚¹ã‚­ãƒƒãƒ—ã€æ›¸é¡å–å¾—APIã®ã¿ï¼‰
        try:
            doc_list = load_doc_id_csv(doc_id_csv_path)
        except FileNotFoundError as e:
            print(f"âŒ {e}")
            sys.exit(1)
        print(f"ğŸ“‹ DocIDå¯¾å¿œè¡¨èª­ã¿è¾¼ã¿: {len(doc_list)} ç¤¾ï¼ˆæ—¥ä»˜æ¤œç´¢ã‚¹ã‚­ãƒƒãƒ—ï¼‰")
        total = 0
        for idx, (sec, doc_id, parent) in enumerate(doc_list):
            if sec in processed:
                continue
            if idx > 0 and idx % BATCH_BREAK_COMPANIES == 0:
                print(f"  â¸  {BATCH_BREAK_SEC}ç§’ä¼‘æ†©ï¼ˆ{idx}ç¤¾å‡¦ç†æ¸ˆã¿ï¼‰")
                time.sleep(BATCH_BREAK_SEC)
            _request_sleep()
            zip_bytes, fmt = fetch_document_zip_with_fallback(doc_id)
            if not zip_bytes:
                print(f"[{idx + 1}/{len(doc_list)}] {sec} ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—")
                save_processed_code(sec)
                continue
            do_debug = args.debug and (idx < 1 or len(doc_list) == 1)
            relations = extract_relations(zip_bytes, parent, debug=do_debug, fetched_format=fmt)
            seen = set()
            unique = [r for r in relations if r.subsidiary_name and (k := (r.parent_name, r.subsidiary_name, r.address, r.voting_rights_pct)) not in seen and not seen.add(k)]
            if unique:
                append_to_csv(unique, output_path)
                total += len(unique)
                print(f"[{idx + 1}/{len(doc_list)}] {sec} â†’ {len(unique)}ä»¶ä¿å­˜")
            else:
                print(f"[{idx + 1}/{len(doc_list)}] {sec} â†’ é–¢ä¿‚ä¼šç¤¾ãƒ‡ãƒ¼ã‚¿ãªã—")
            save_processed_code(sec)
    else:
        # ä¸€æ‹¬ã‚­ãƒ£ãƒƒã‚·ãƒ¥â†’ã‚ªãƒ•ãƒ©ã‚¤ãƒ³æ¤œç´¢â†’ãƒ”ãƒ³ãƒã‚¤ãƒ³ãƒˆå–å¾—
        if args.clear_cache:
            cache_dir = Path(EDINET_CACHE_DIR)
            if cache_dir.exists():
                shutil.rmtree(cache_dir)
                print(f"ğŸ—‘ data/cache/ ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
            cache_dir.mkdir(parents=True, exist_ok=True)

        sec_set = {str(c).zfill(4) for c in codes}
        if args.all_listed and args.use_fiscal:
            try:
                fiscal_list = load_listed_with_fiscal()
                fiscal_map = {sec: fm for sec, fm in fiscal_list if sec in sec_set}
                dates_to_fetch = build_dates_from_fiscal(list(fiscal_map.items()))
                print(f"ğŸ“… æ¤œç´¢å¯¾è±¡: æ±ºç®—æ—¥ãƒ™ãƒ¼ã‚¹çµã‚Šè¾¼ã¿ {len(dates_to_fetch)} æ—¥åˆ†")
            except Exception as e:
                print(f"âš ï¸ æ±ºç®—æ—¥èª­ã¿è¾¼ã¿å¤±æ•—ã€--days ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {e}")
                dates_to_fetch = [(datetime.now().date() - timedelta(days=i)).strftime("%Y-%m-%d") for i in range(args.days)]
                print(f"ğŸ“… æ¤œç´¢å¯¾è±¡: éå»{args.days}æ—¥åˆ†")
        else:
            dates_to_fetch = [(datetime.now().date() - timedelta(days=i)).strftime("%Y-%m-%d") for i in range(args.days)]
            print(f"ğŸ“… æ¤œç´¢å¯¾è±¡: éå»{args.days}æ—¥åˆ†ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥: {'ç„¡åŠ¹' if args.no_cache else 'æœ‰åŠ¹'}ï¼‰")
        use_cache = not args.no_cache and not args.force_refresh_cache
        if args.force_refresh_cache:
            print("ğŸ”„ --force-refresh-cache: æ—¢å­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡è¦–ã—ã€æ›¸é¡ä¸€è¦§ã‚’v2ã‹ã‚‰å†å–å¾—ã—ã¾ã™")
        ensure_dates_cached(dates_to_fetch, use_cache=use_cache)
        doc_map = build_sec_to_doc_map_from_full_cache(sec_set)
        cache_count = len(list(Path(EDINET_CACHE_DIR).glob("*.json"))) if Path(EDINET_CACHE_DIR).exists() else 0
        print(f"   æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸(docTypeCode=120): {len(doc_map)} ç¤¾åˆ†ã‚’æ¤œå‡ºï¼ˆå…¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±åˆã‚¹ã‚­ãƒ£ãƒ³ã€{cache_count} ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰")

        # --debug æ™‚ã¯1ç¤¾ã«çµã£ã¦è©³ç´°è§£æ
        codes_to_process = codes[:1] if args.debug and codes else codes
        if args.debug and len(codes) > 1:
            print(f"ğŸ” --debug: 1ç¤¾ã«çµã£ã¦è©³ç´°è§£æï¼ˆ{codes[0]}ï¼‰")

        for idx, sec in enumerate(codes_to_process):
            sec4 = str(sec).zfill(4)
            doc_info = doc_map.get(sec4)
            if not doc_info:
                print(f"[{idx + 1}/{len(codes_to_process)}] {sec} æ¤œç´¢ä¸­... æœªç™ºè¦‹")
                # 1å¹´åˆ†ã®å…¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã¦æœ‰å ±ãŒå­˜åœ¨ã—ãªã‹ã£ãŸå ´åˆã®ã¿å‡¦ç†æ¸ˆã¿ã«ã™ã‚‹
                save_processed_code(sec)
                continue

            doc_id = doc_info.get("docID")
            parent = doc_info.get("filerName") or f"è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰{sec}"

            # 100ä¼æ¥­ã”ã¨ã«ä¼‘æ†©ï¼ˆä¼æ¥­æ•°ãƒ™ãƒ¼ã‚¹ï¼‰
            if idx > 0 and idx % BATCH_BREAK_COMPANIES == 0:
                print(f"  â¸  {BATCH_BREAK_SEC}ç§’ä¼‘æ†©ï¼ˆ{idx}ç¤¾å‡¦ç†æ¸ˆã¿ï¼‰")
                time.sleep(BATCH_BREAK_SEC)

            _request_sleep()
            zip_bytes, fmt = fetch_document_zip_with_fallback(doc_id)
            if not zip_bytes:
                print(f"[{idx + 1}/{len(codes_to_process)}] {sec} æ¤œç´¢ä¸­... ç™ºè¦‹ â†’ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—")
                # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—æ™‚ã¯å‡¦ç†æ¸ˆã¿ã«ã—ãªã„ï¼ˆæ¬¡å›ãƒªãƒˆãƒ©ã‚¤ã™ã‚‹ï¼‰
                continue

            relations = extract_relations(zip_bytes, parent, debug=args.debug, fetched_format=fmt)
            seen = set()
            unique = []
            for r in relations:
                key = (r.parent_name, r.subsidiary_name, r.address, r.voting_rights_pct)
                if key not in seen and r.subsidiary_name:
                    seen.add(key)
                    unique.append(r)

            if unique:
                append_to_csv(unique, output_path)
                total += len(unique)
                print(f"[{idx + 1}/{len(codes_to_process)}] {sec} æ¤œç´¢ä¸­... ç™ºè¦‹ â†’ {len(unique)}ä»¶ä¿å­˜")
                save_processed_code(sec)
            else:
                print(f"[{idx + 1}/{len(codes_to_process)}] {sec} æ¤œç´¢ä¸­... ç™ºè¦‹ â†’ é–¢ä¿‚ä¼šç¤¾ãƒ‡ãƒ¼ã‚¿ãªã—")
                # æœ‰å ±ã¯è¦‹ã¤ã‹ã£ãŸãŒæŠ½å‡º0ä»¶ã®å ´åˆã¯å‡¦ç†æ¸ˆã¿ã«ã—ãªã„ï¼ˆæŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯æ”¹å–„å¾Œã«ãƒªãƒˆãƒ©ã‚¤ï¼‰

    print(f"\nğŸ“ å‡ºåŠ›: {output_path}")
    print(f"ğŸ“Š åˆè¨ˆ {total} ä»¶ã‚’è¿½åŠ ã—ã¾ã—ãŸ")


if __name__ == "__main__":
    main()
